{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub\n",
        "import kagglehub\n",
        "import os\n",
        "\n",
        "print(\"Downloading dataset...\")\n",
        "path = kagglehub.dataset_download(\"emmarex/plantdisease\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "directory_root = os.path.join(path, \"PlantVillage\")\n",
        "print(\"Path to images:\", directory_root)"
      ],
      "metadata": {
        "id": "FrTpkkxDYmLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define constants for the data pipeline\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create the main training dataset (80%)\n",
        "print(\"Creating training dataset...\")\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory_root,\n",
        "    validation_split=0.2,  # Reserve 20% for validation and testing\n",
        "    subset=\"training\",\n",
        "    seed=42,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Create the temporary hold-out dataset (the 20%)\n",
        "print(\"Creating validation/test dataset...\")\n",
        "temp_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory_root,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=42,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Get the class names and count\n",
        "class_names = train_ds.class_names\n",
        "n_classes = len(class_names)\n",
        "print(f\"Found {n_classes} classes.\")\n",
        "\n",
        "# Split the 20% hold-out into 10% validation and 10% test\n",
        "val_batches = tf.data.experimental.cardinality(temp_ds) // 2\n",
        "val_ds = temp_ds.take(val_batches)\n",
        "test_ds = temp_ds.skip(val_batches)\n",
        "\n",
        "print(\"Optimizing data pipelines...\")\n",
        "train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"Data pipelines created successfully.\")"
      ],
      "metadata": {
        "id": "iwbLT-gYYtpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow\n",
        "from tensorflow.keras.applications import ResNet50, EfficientNetB0\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Rescaling, RandomFlip, RandomRotation, RandomZoom\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from os import listdir\n",
        "\n",
        "# --- Constants ---\n",
        "EPOCHS = 30\n",
        "INIT_LR = 1e-4\n",
        "width = 224\n",
        "height = 224\n",
        "depth = 3\n",
        "FINE_TUNE_AT_EPOCH = 15\n",
        "FINE_TUNE_LR = INIT_LR / 10 # 1e-5\n",
        "\n",
        "# --- Preprocessing Layer ---\n",
        "data_preprocessing = Sequential(\n",
        "  [\n",
        "    Rescaling(1./255), # Normalization\n",
        "    RandomFlip(\"horizontal\"),\n",
        "    RandomRotation(0.1),\n",
        "    RandomZoom(0.2),\n",
        "  ],\n",
        "  name=\"data_preprocessing\",\n",
        ")\n",
        "\n",
        "# --- Build Model Function ---\n",
        "def build_model(architecture_name, n_classes, input_shape):\n",
        "    \"\"\"Builds ResNet50 or EfficientNet model with Transfer Learning.\"\"\"\n",
        "\n",
        "    if architecture_name == 'ResNet50':\n",
        "        base_model = ResNet50(weights=\"imagenet\", include_top=False,\n",
        "                              input_shape=input_shape)\n",
        "    elif architecture_name == 'EfficientNet':\n",
        "        base_model = EfficientNetB0(weights=\"imagenet\", include_top=False,\n",
        "                                      input_shape=input_shape)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid architecture name\")\n",
        "\n",
        "    # Freeze the base layers\n",
        "    base_model.trainable = False\n",
        "\n",
        "    x = data_preprocessing(base_model.input)\n",
        "    x = base_model(x, training=False)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation=\"relu\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    predictions = Dense(n_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    return model, base_model\n",
        "\n",
        "# --- Model Creation ---\n",
        "inputShape = (height, width, depth) # Run one at a time\n",
        "model, base_model = build_model('ResNet50', n_classes, inputShape)\n",
        "# model, base_model = build_model('EfficientNet', n_classes, inputShape)\n",
        "\n",
        "# --- ================== STEP 1: WARM-UP TRAINING ================== ---\n",
        "print(\"[INFO] STEP 1: Warming up the model head...\")\n",
        "\n",
        "opt = Adam(learning_rate=INIT_LR)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "# Train for the first 15 epochs\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=FINE_TUNE_AT_EPOCH,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --- ================== STEP 2: FINE-TUNE TRAINING ================== ---\n",
        "print(\"\\n[INFO] STEP 2: Unfreezing and fine-tuning the model...\")\n",
        "\n",
        "# Unfreeze the base model\n",
        "base_model.trainable = True\n",
        "\n",
        "opt_fine_tune = Adam(learning_rate=FINE_TUNE_LR)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt_fine_tune, metrics=[\"accuracy\"])\n",
        "\n",
        "# Continue training from where we left off\n",
        "# The model will train for (EPOCHS - FINE_TUNE_AT_EPOCH) more epochs\n",
        "history_fine_tune = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS, # Total epochs to run to\n",
        "    initial_epoch=history.epoch[-1], # Start from the last epoch number\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --- ================== ANALYSIS AND SAVING ================== ---\n",
        "\n",
        "# Combine the history objects for plotting\n",
        "acc = history.history['accuracy'] + history_fine_tune.history['accuracy']\n",
        "val_acc = history.history['val_accuracy'] + history_fine_tune.history['val_accuracy']\n",
        "loss = history.history['loss'] + history_fine_tune.history['loss']\n",
        "val_loss = history.history['val_loss'] + history_fine_tune.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# Plotting\n",
        "print(\"Plotting training history...\")\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate\n",
        "print(\"[INFO] Calculating model accuracy on test set\")\n",
        "scores = model.evaluate(test_ds)\n",
        "print(f\"Test Accuracy: {scores[1]*100}\")\n",
        "\n",
        "# Save Model\n",
        "print(\"[INFO] Saving model...\")\n",
        "model.save('cnn_model.keras')\n",
        "\n",
        "print(\"Saving class names...\")\n",
        "with open('label_transform.pkl', 'wb') as f:\n",
        "    pickle.dump(class_names, f)"
      ],
      "metadata": {
        "id": "OaasUpINlJiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. Get True Labels and Predictions ---\n",
        "\n",
        "print(\"[INFO] Loading saved model...\")\n",
        "\n",
        "model = tf.keras.models.load_model('cnn_model.keras')\n",
        "\n",
        "print(\"[INFO] Evaluating model on the test dataset...\")\n",
        "\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "for images, labels in test_ds:\n",
        "    predictions = model.predict(images, verbose=0)\n",
        "    y_pred.extend(np.argmax(predictions, axis=1))\n",
        "    y_true.extend(labels.numpy())\n",
        "\n",
        "print(\"Evaluation complete.\")\n",
        "\n",
        "# --- 2. Print Classification Report ---\n",
        "\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "print(\"\\nGenerating confusion matrix...\")\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names)\n",
        "plt.title('Confusion Matrix (ResNet50)')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WxSOGNu4X0G-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}